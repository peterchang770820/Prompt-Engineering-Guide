## 可靠性

我們已經看到精心設計的提示對於使用像小樣本學習這樣的技術的各種任務是多麼有效。當我們考慮在 LLM 之上構建真實世界的應用程序時，考慮這些語言模型的可靠性變得至關重要。本指南重點展示有效的提示技術，以提高 GPT-3 等 LLM 的可靠性。一些感興趣的話題包括普遍性、校準、偏見、社會偏見和事實性等等。

**請注意，此部分正在大力開發中。**

主題：
- [事實]（#factuality）
- [偏見](#biases)
- ...

---
## 事實
LLM 傾向於生成聽起來連貫且令人信服但有時可以彌補的響應。改進提示有助於改進模型以生成更準確/符合事實的響應，並降低生成不一致和編造響應的可能性。

一些解決方案可能包括：
- 提供基本事實（例如，相關文章段落或維基百科條目）作為上下文的一部分，以減少模型生成虛構文本的可能性。
- 通過降低概率參數並指示它在不知道答案時承認（例如，“我不知道”），將模型配置為產生較少多樣性的響應。
- 在提示中提供它可能知道和不知道的問題和答案的組合示例

讓我們看一個簡單的例子：

*迅速的：*
```
問：什麼是原子？
A：原子是構成一切的微小粒子。

問：Alvan Muntz 是誰？
A： ？

問：什麼是 Kozar-09？
A： ？問：

火星有多少顆衛星？
A：兩個，火衛一和火衛二。

問：Neto Beto Roberto 是誰？
```

*輸出：*
```
A： ？
```

我編了個名字“Neto Beto Roberto”，所以這個例子中的模型是正確的。試著稍微改變一下這個問題，看看你能否讓它發揮作用。根據您目前所學的所有知識，您可以通過不同的方式進一步改進這一點。

---
## 偏見
LLM 可能會產生有問題的生成，這些生成可能是有害的，並且會顯示出可能會降低模型在下游任務上的性能的偏差。其中一些可以通過有效的提示策略來緩解，但可能需要更高級的解決方案，如適度和過濾。

### 範例分佈
在進行小樣本學習時，樣本的分佈是否會影響模型的性能或以某種方式使模型產生偏差？我們可以在這裡進行一個簡單的測試。

*迅速的：*
```
問：我剛剛得到最好的消息！
答：正面

問：我們剛加薪！
答：正面

問：我為今天所取得的成就感到非常自豪。
答：正面

問：我度過了最美好的一天！
答：正面

問：我真的很期待週末。
答：正面

問：我剛剛收到最好的禮物！
答：正面

問：我現在很開心。
答：正面

問：我很幸運有這樣一個很棒的家庭。
答：正面

問：外面的天氣很陰沉。
答：否定

問：我剛得到一些可怕的消息。
答：否定

問：那留下了酸味。
A：
```

*輸出：*
```
消極的
```

在上面的例子中，樣本的分佈似乎並沒有使模型產生偏差。這很好。讓我們嘗試另一個帶有更難分類文本的示例，讓我們看看模型是如何工作的：

*迅速的：*
```
問：這裡的食物很好吃！
答：正面

問：我厭倦了這門課程。
答：否定

問：我不敢相信我沒有通過考試。
答：否定

問：我今天過得很愉快！
答：正面

問：我討厭這份工作。
答：否定

問：這裡的服務很糟糕。
答：否定

問：我對自己的生活感到很沮喪。
答：否定

問：我從來沒有休息過。
答：否定

問：這頓飯很難吃。
答：否定

問：我無法忍受我的老闆。
答：否定

問：我感覺到了一些東西。
A：
```

*輸出：*
```
消極的
```

雖然最後一句話有些主觀，但我翻轉了分佈，改為使用 8 個正例和 2 個負例，然後再次嘗試完全相同的句子。猜猜模特的反應是什麼？它回答“積極”。該模型可能有很多關於情感分類的知識，因此很難讓它顯示出對這個問題的偏見。這裡的建議是避免扭曲分佈，而是為每個標籤提供更均衡的示例數量。對於模型沒有太多知識的更艱鉅的任務，它可能會更加掙扎。


### 示例順序
在進行小樣本學習時，順序是否會影響模型的性能或以某種方式使模型產生偏差？

您可以嘗試上述示例，看看是否可以通過更改順序使模型偏向標籤。建議是隨機排序樣本。例如，避免先有所有的正面例子，然後才是負面的例子。如果標籤的分佈是傾斜的，這個問題會進一步放大。始終確保進行大量實驗以減少此類偏見。

---

其他即將到來的話題：
- 擾動
- 虛假相關
- 域轉移
- 毒性
- 刻板印象偏見
- 性別偏見
- 即將推出！
- 紅隊

---
＃＃ 參考
- [憲法 AI：AI 反饋的無害性](https://arxiv.org/abs/2212.08073)（2022 年 12 月）
- [重新思考演示的作用：是什麼讓情境學習發揮作用？](https://arxiv.org/abs/2202.12837)（2022 年 10 月）
- [提示 GPT-3 可靠](https://arxiv.org/abs/2210.09150)（2022 年 10 月）
- [使語言模型成為更好的推理者的進展](https://arxiv.org/abs/2206.02336)（2022 年 6 月）
- [機器學習安全中未解決的問題](https://arxiv.org/abs/2109.13916)（2021 年 9 月）
- [減少危害的紅隊語言模型：方法、擴展行為和經驗教訓](https://arxiv.org/abs/2209.07858)（2022 年 8 月）
- [StereoSet：測量預訓練語言模型中的刻板印象偏見](https://aclanthology.org/2021.acl-long.416/)（2021 年 8 月）
- [使用前校準：提高語言模型的小樣本性能](https://arxiv.org/abs/2102.09690v2)（2021 年 2 月）
- [提高可靠性的技術 - OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)

---
[上一節（對抗性提示）](./prompts-adversarial.md)

[下一節（雜項）](./prompts-miscellaneous.md)